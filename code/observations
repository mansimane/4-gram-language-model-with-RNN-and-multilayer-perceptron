 #bs_256_lr_0.01_hl_ 128_ev_16_ep_100
[218.47193562909629, 5.4232549204559524, 197.88215182588294, 5.3235964982704047]


######## Config 1
hyper_para['epochs'] = 100

hyper_para['batch_size'] = 512

hyper_para['hidden_layer_size'] = 128   # 100 hidden units

hyper_para['decay'] = 0.0005      # weight decay

hyper_para['embed_size'] = 16   # Size of embedding vector

hyper_para['vocab_size'] = 8000   # size of vocabulary

hyper_para['no_of_grams'] = 4

hyper_para['learning_rate'] = 0.1
hyper_para['context_size'] = hyper_para['no_of_grams'] - 1

hyper_para['w_init_mu'] = 0
hyper_para['w_init_sig'] = 0.01 # mean and standard deviation

hyper_para['c_init_mu'] = 0
hyper_para['c_init_sig'] = 0.05 # mean and standard deviation







